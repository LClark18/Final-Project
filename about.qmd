---
title: "Data Wrangling and Modeling"
---


## Packages Used In This Analysis


```{r}
#| label: load packages
#| message: false
#| warning: false

library(GGally)
library(tidyverse)
library(tidyclust)
library(tidymodels)
library(dplyr)
library(ggplot2)
library(naniar)
```

GGally to get the ggpairs function

tidyclust to have a tidy interface for clustering models

dplyr to massage and summarize data

tidymodels for modeling

tidyverse for piping and altering data

ggplot2 for graphs in eda and clustering

naniar to visualize missing data

## Data Description

```{r}
#| label: import data
#| warning: false
vgsales <- readr::read_csv("C:/Users/19493/Desktop/vgsales.csv")
```

I got the dataset from a site called Kaggle.com and the data set analyzes sales data from over 16,000 games that had more than 100,000 global sales. The data itself was generated by a scrape of vgchartz.com, which is another website that goes into deep analysis about video games sales. 

The Data has 11 different variables Rank is a games placement in sales, Name is the name of the game, Platform is which console said game came out on, Genre is the genre of the game, Publisher is who published the game, and the different Sales columns relate to how many copies the game sole in North America, Europe, Japan, all other territories, and final how many total copies were sold globally. 

### Data Limitations

The data only contains data until about 2016, so the data set itself is old and needs to be updated. There are a lot of outliers in the data itself, and about 2% of the year values are missing.

## Data Wrangling

```{r}
vgsales2 <- vgsales %>%
  filter(Global_Sales > 1) %>%
  mutate(log_sales = log(Global_Sales)) %>%
mutate(NA_log_sales = log(NA_Sales + 0.0001)) %>%
mutate(EU_log_sales = log(EU_Sales + 0.0001)) %>%
mutate(JP_log_sales = log(JP_Sales + 0.0001)) %>%
mutate(OT_log_sales = log(Other_Sales + 0.0001)) %>%
  mutate(Platform = as.factor(Platform) |>
fct_collapse(
  Sony = c("PS", "PS2", "PS3", "PS4", "PSP", "PSV"),
  Microsoft = c("X360", "XB", "XOne"),
  Nintendo = c("NES", "SNES", "Wii", "WiiU", "GC", "N64", "3DS", "DS", "GB", "GBA"),
  Other = c("2600", "DC", "GEN", "PC", "SAT", "SCD")
))

vg_split <- initial_split(vgsales2, prop = 0.75) 

vg_train <- training(vg_split)
vg_test <- testing(vg_split)
```

Here we have to makes some changes to the data so that our model later can run, and to help with outliers, we'll call this new data something different so we can compare later. The main actions were to log all the Sales columns so that we wouldn't have as many outliers, and to add a very small amount to the sales number since some values show up as 0.00, and that would mean if we log them we were get negative infinity. I also chose to include only games that sold a minimum of one million copies world wide so that we can see what similarities we see in games considered a success. We also change our Platform variable to only include 4 different values again so that our model can run later on. We can also split our data to see how well our training clusters do with the test set.

## EDA

```{r}
vgsales$Year[vgsales$Year == "N/A"] <- NA
vgsales$Publisher[vgsales$Publisher == "N/A"] <- NA

library(naniar)

vis_miss(vgsales[1:11])
```


```{r}
vgsales2$Year[vgsales2$Year == "N/A"] <- NA
vgsales2$Publisher[vgsales2$Publisher == "N/A"] <- NA
vis_miss(vgsales2[1:11])
```

These plots above show the missing data from our original data set and our wrangled data set. As we can see originally we had 2% of years missing and <1% of Publisher missing. Now with our wrangled data we see that we only have ~1% of data missing from year. This doesn't really affect our model later as we're not going to use the year function to build it, but still important to know.


```{r}
ggplot(data = vg_train,
       aes(x = Year, y = Global_Sales)) + geom_boxplot()
```


```{r}
ggplot(data = vg_train,
       aes(x = Year, y = log_sales)) + geom_boxplot()
```

As we can see from the training data, the number of outliers in here is high, hence why earlier we took the log of the global sales column and made it it's own variable. Our 1st graph shows the global sales unlogged and the second shows log sales, less outliers than before.

```{r}
ggplot(data = vg_train,
       aes(y = Global_Sales)) + geom_point(aes(x = NA_Sales), color = "red") + geom_smooth(aes(x = NA_Sales), color = "red", method = "loess") + geom_point(aes(x = EU_Sales), color = "green") + geom_smooth(aes(x = EU_Sales), color = "green", method = "loess") + geom_point(aes(x = JP_Sales), color = "blue") + geom_smooth(aes(x = JP_Sales), color = "blue", method = "loess") + geom_point(aes(x = Other_Sales), color = "black") + geom_smooth(aes(x = Other_Sales), color = "black", method = "loess") + labs(x = "Sales by Region", y = "Global Sales")
```

Here is a graph I created to show a relationship between all 4 different regions and how they related to the total global sales

## Modeling


```{r}
kmeans_recipe_fv <- recipe(~ log_sales + NA_log_sales + EU_log_sales + JP_log_sales + OT_log_sales + Platform + Genre, 
                           data = vg_train) |>
  step_YeoJohnson(all_numeric_predictors()) |> # deal with skew issues
  step_normalize(all_numeric_predictors()) |> # deal with different variances
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors())
```

This is the recipe used for Kmeans clustering

```{r}
kmeans_model <- k_means(num_clusters = tune()) |>
  set_args(nstart = 20)
```

The cluster model

```{r}
kmeans_wflow_fv <- workflow() |>
  add_model(kmeans_model) |>
  add_recipe(kmeans_recipe_fv)
```

```{r}
set.seed(1002)
fv_kfold_tidy <- vfold_cv(vg_train, v = 5, repeats = 1) 
nclusters_grid <- data.frame(num_clusters = seq(1, 10))

kmeans_tuned_fv <- tune_cluster(kmeans_wflow_fv,
                                resamples = fv_kfold_tidy,
                                metrics = cluster_metric_set(sse_total, 
                                                             sse_within_total, sse_ratio),
                                grid = nclusters_grid)

tuned_metrics <- collect_metrics(kmeans_tuned_fv)

tuned_metrics |>
  arrange(desc(.metric), num_clusters) |>
  select(num_clusters, .metric, mean, everything())
```

```{r}
tuned_metrics |>
  filter(.metric == "sse_ratio") |>
  ggplot(aes(x = num_clusters, y = mean)) +
  geom_point() + 
  geom_line() +
  labs(x = "Number of Clusters", y = "Mean WSS/TSS (5 folds)") +
  scale_x_continuous(breaks = seq(1, 10))
```

```{r}
kmeans_fv_3clusters <- kmeans_wflow_fv |>
  finalize_workflow_tidyclust(parameters = list(num_clusters = 3))
```

```{r}
set.seed(1002) 
# always reset the seed before you re-fit, just in case something weird happens

kmeans_fv_fit3 <- kmeans_fv_3clusters |>
  fit(data = vg_train)
```

```{r}
vg3 <- bind_cols(
  vg_train,
  kmeans_fv_fit3 |> extract_cluster_assignment())

vg3 |>
  select(Name, .cluster, everything())
```

```{r}
library(GGally)
ggpairs(vg3, columns = c("log_sales", "NA_log_sales", "EU_log_sales", "JP_log_sales", "OT_log_sales", "Platform", "Genre"),
        aes(color = .cluster))
```

```{r}
vg_predictions3 <- augment(kmeans_fv_fit3, 
                        new_data = vg_test)

ggpairs(vg_predictions3, columns = c("log_sales", "NA_log_sales", "EU_log_sales", "JP_log_sales", "OT_log_sales", "Platform", "Genre"),
        aes(color = .pred_cluster))
```

```{r}
vg_all_clusters3 <- bind_rows(
  vg3,
  vg_predictions3 |> rename(.cluster = .pred_cluster) # rename cluster variable name
)
```

## Insights

```{r}
ggplot(data = vg_all_clusters3, mapping = aes(x = log_sales, fill = .cluster)) + geom_density()
```

```{r}
ggplot(data = vg_all_clusters3, mapping = aes(x = NA_log_sales, fill = .cluster)) + geom_density()
```

```{r}
ggplot(data = vg_all_clusters3, mapping = aes(x = EU_log_sales, fill = .cluster)) + geom_density()
```

```{r}
ggplot(data = vg_all_clusters3, mapping = aes(x = JP_log_sales, fill = .cluster)) + geom_density()
```

```{r}
ggplot(data = vg_all_clusters3, mapping = aes(x = OT_log_sales, fill = .cluster)) + geom_density()
```

```{r}
ggplot(data = vg_all_clusters3, mapping = aes(x = Platform, fill = .cluster)) + geom_bar()
```

```{r}
ggplot(data = vg_all_clusters3, mapping = aes(x = Genre, fill = .cluster)) + geom_bar()
```

The thing with clustering is, we have to assign each of these clusters a meaning
Cluster 1 - Games that sold alright in most regions besides Japan
Cluster 2 - The Games that had the most sales in most regions
Cluster 3 - Same as cluster 1, except games sold incredibly well in Japan

```{r}
cluster1 <- filter(vg_all_clusters3, .cluster == "Cluster_1")
cluster2 <- filter(vg_all_clusters3, .cluster == "Cluster_2")
cluster3 <- filter(vg_all_clusters3, .cluster == "Cluster_3")

n1 <- nrow(cluster1)
n2 <- nrow(cluster2)
n3 <- nrow(cluster3)

all_nintendo_1 <- dplyr::filter(cluster1, Platform %in% "Nintendo")
all_nintendo_2 <- dplyr::filter(cluster2, Platform %in% "Nintendo")
all_nintendo_3 <- dplyr::filter(cluster3, Platform %in% "Nintendo")
all_nintendo <- dplyr::filter(vg_all_clusters3, Platform %in% "Nintendo")
nrow(all_nintendo_1)/nrow(all_nintendo)
nrow(all_nintendo_2)/nrow(all_nintendo)
nrow(all_nintendo_3)/nrow(all_nintendo)
```

```{r}
all_sony_1 <- dplyr::filter(cluster1, Platform %in% "Sony")
all_sony_2 <- dplyr::filter(cluster2, Platform %in% "Sony")
all_sony_3 <- dplyr::filter(cluster3, Platform %in% "Sony")
all_sony <- dplyr::filter(vg_all_clusters3, Platform %in% "Sony")
nrow(all_sony_1)/nrow(all_sony)
nrow(all_sony_2)/nrow(all_sony)
nrow(all_sony_3)/nrow(all_sony)
```
```{r}
all_Microsoft_1 <- dplyr::filter(cluster1, Platform %in% "Microsoft")
all_Microsoft_2 <- dplyr::filter(cluster2, Platform %in% "Microsoft")
all_Microsoft_3 <- dplyr::filter(cluster3, Platform %in% "Microsoft")
all_Microsoft <- dplyr::filter(vg_all_clusters3, Platform %in% "Microsoft")
nrow(all_Microsoft_1)/nrow(all_Microsoft)
nrow(all_Microsoft_2)/nrow(all_Microsoft)
nrow(all_Microsoft_3)/nrow(all_Microsoft)
```
The platform most likely to have their games sell well Japan is Nintendo with over 46% percent of their games being in cluster 3.
The platform most likely to have their games sell the best world wide is Sony with almost 30% of their games being in cluster 2, higher than Nintendo and Microsoft.
The games most likely to sell the best in Japan seem to being games published on Nintendo Consoles that are RPG's, with cluster 3 taking up over half of the RPG's games
The games most likely to sell best worldwide look to be Adventure Games released on Sony Consoles.

##Limitations
With Clustering it's more of how we determine it, because the very nature of clustering means we have to give each cluster a mean so it's very suseptible to human error
