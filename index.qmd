---
title: "Logan Clark 437 Final"
subtitle: "4th Year Student"
image: Logan_Clark.jpg
about: 
  template: jolla
  
links:
  - icon: github
    text: Github
    href: https://github.com/LClark18
---

Hello my name is Logan Clark and I'm a 4th year here at Cal State University Fullerton and this is my final project for my Math 437: Modern Data Analysis class.

## Motivation and Context

```{r}
#| label: do this first
#| echo: false
#| message: false

# change this to the location of your Quarto file containing your project; then delete this comment
here::i_am("index.qmd")
```

For my project I decided to look at the sales of Video Games. Video Games have been my main hobby for most of my life and I've always had an innate interest in sales numbers. I would go on Wikipedia just to look at the list of best selling games and see what changes would be happening on the chart on a weekly basis. Each game would have it's own entry and show us what console it came out on, when it came out, etc. It makes me wonder if there's any common trends among games that have higher sales overall, such as if a specific genre is more appealing, or if games on a certain console did better than others. 

## Main Objective

My main Objective for this project is to use clustering to see what games are similar and to see if we can see any patterns among games in the cluster that contains higher global sales.

## Packages Used In This Analysis

```{r}
#| label: load packages
#| message: false
#| warning: false

library(GGally)
library(tidyverse)
library(tidyclust)
library(tidymodels)
library(dplyr)
library(ggplot2)
```

| Package | Use |
|-------------------------------|----------------------------------------|
| [GGally](https://cran.r-project.org/web/packages/GGally/index.html) | to easily load and save data |
| [tidyverse](https://cran.r-project.org/web/packages/tidyverse/index.html) | to import the CSV file data |
| [tidyclust](https://cran.r-project.org/web/packages/tidyclust/index.html) | to massage and summarize data |
| [tidymodels](https://cran.r-project.org/web/packages/tidymodels/index.html) | to split data into training and test sets |
| [dplyr](https://dplyr.tidyverse.org/) | to massage and summarize data |
| [ggplot2](https://ggplot2.tidyverse.org/) | to create nice-looking and informative graphs |
| [nanier](https://cran.r-project.org/web/packages/naniar/index.html) | to visualize missing data |


## Data Description

```{r}
#| label: import data
#| warning: false
vgsales <- readr::read_csv("C:/Users/19493/Desktop/vgsales.csv")
```

I got the dataset from a site called Kaggle.com and the data set analyzes sales data from over 16,000 games that had more than 100,000 global sales. The data itself was generated by a scrape of vgchartz.com, which is another website that goes into deep analysis about video games sales. 

The Data has 11 different variables Rank is a games placement in sales, Name is the name of the game, Platform is which console said game came out on, Genre is the genre of the game, Publisher is who published the game, and the different Sales columns relate to how many copies the game sole in North America, Europe, Japan, all other territories, and final how many total copies were sold globally. 

### Data Limitations

The data only contains data until about 2016, so the data set itself is old and needs to be updated. There are a lot of outliers in the data itself, and about 2% of the year values are missing.

## Data Wrangling

```{r}
vgsales2 <- vgsales %>%
  filter(Global_Sales > 1) %>%
  mutate(log_sales = log(Global_Sales)) %>%
mutate(NA_log_sales = log(NA_Sales + 0.0001)) %>%
mutate(EU_log_sales = log(EU_Sales + 0.0001)) %>%
mutate(JP_log_sales = log(JP_Sales + 0.0001)) %>%
mutate(OT_log_sales = log(Other_Sales + 0.0001)) %>%
  mutate(Platform = as.factor(Platform) |>
fct_collapse(
  Sony = c("PS", "PS2", "PS3", "PS4", "PSP", "PSV"),
  Microsoft = c("X360", "XB", "XOne"),
  Nintendo = c("NES", "SNES", "Wii", "WiiU", "GC", "N64", "3DS", "DS", "GB", "GBA"),
  Other = c("2600", "DC", "GEN", "PC", "SAT", "SCD")
))

vg_split <- initial_split(vgsales2, prop = 0.75) 

vg_train <- training(vg_split)
vg_test <- testing(vg_split)
```

Here we have to makes some changes to the data so that our model later can run, and to help with outliers, we'll call this new data something different so we can compare later. The main actions were to log all the Sales columns so that we wouldn't have as many outliers, and to add a very small amount to the sales number since some values show up as 0.00, and that would mean if we log them we were get negative infinity. I also chose to include only games that sold a minimum of one million copies world wide so that we can see what similarities we see in games considered a success. We also change our Platform variable to only include 4 different values again so that our model can run later on. We can also split our data to see how well our training clusters do with the test set.

## EDA

```{r}
vgsales$Year[vgsales$Year == "N/A"] <- NA
vgsales$Publisher[vgsales$Publisher == "N/A"] <- NA

library(naniar)

vis_miss(vgsales[1:11])
```


```{r}
vgsales2$Year[vgsales2$Year == "N/A"] <- NA
vgsales2$Publisher[vgsales2$Publisher == "N/A"] <- NA
vis_miss(vgsales2[1:11])
```

These plots above show the missing data from our original data set and our wrangled data set. As we can see originally we had 2% of years missing and <1% of Publisher missing. Now with our wrangled data we see that we only have ~1% of data missing from year. This doesn't really affect our model later as we're not going to use the year function to build it, but still important to know.


```{r}
ggplot(data = vg_train,
       aes(x = Year, y = Global_Sales)) + geom_boxplot()
```


```{r}
ggplot(data = vg_train,
       aes(x = Year, y = log_sales)) + geom_boxplot()
```

As we can see from the training data, the number of outliers in here is high, hence why earlier we took the log of the global sales column and made it it's own variable. Our 1st graph shows the global sales unlogged and the second shows log sales, less outliers than before.

```{r}
ggplot(data = vg_train,
       aes(y = Global_Sales)) + geom_point(aes(x = NA_Sales), color = "red") + geom_smooth(aes(x = NA_Sales), color = "red", method = "loess") + geom_point(aes(x = EU_Sales), color = "green") + geom_smooth(aes(x = EU_Sales), color = "green", method = "loess") + geom_point(aes(x = JP_Sales), color = "blue") + geom_smooth(aes(x = JP_Sales), color = "blue", method = "loess") + geom_point(aes(x = Other_Sales), color = "black") + geom_smooth(aes(x = Other_Sales), color = "black", method = "loess") + labs(x = "Sales by Region", y = "Global Sales")
```

## Modeling


```{r}
kmeans_recipe_fv <- recipe(~ log_sales + NA_log_sales + EU_log_sales + JP_log_sales + OT_log_sales + Platform + Genre, 
                           data = vg_train) |>
  step_YeoJohnson(all_numeric_predictors()) |> # deal with skew issues
  step_normalize(all_numeric_predictors()) |> # deal with different variances
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors())
```

This is the recipe used for Kmeans clustering

```{r}
kmeans_model <- k_means(num_clusters = tune()) |>
  set_args(nstart = 20)
```

The cluster model

```{r}
kmeans_wflow_fv <- workflow() |>
  add_model(kmeans_model) |>
  add_recipe(kmeans_recipe_fv)
```

```{r}
set.seed(1002)
fv_kfold_tidy <- vfold_cv(vg_train, v = 5, repeats = 1) 
nclusters_grid <- data.frame(num_clusters = seq(1, 10))

kmeans_tuned_fv <- tune_cluster(kmeans_wflow_fv,
                                resamples = fv_kfold_tidy,
                                metrics = cluster_metric_set(sse_total, 
                                                             sse_within_total, sse_ratio),
                                grid = nclusters_grid)

tuned_metrics <- collect_metrics(kmeans_tuned_fv)

tuned_metrics |>
  arrange(desc(.metric), num_clusters) |>
  select(num_clusters, .metric, mean, everything())
```

```{r}
tuned_metrics |>
  filter(.metric == "sse_ratio") |>
  ggplot(aes(x = num_clusters, y = mean)) +
  geom_point() + 
  geom_line() +
  labs(x = "Number of Clusters", y = "Mean WSS/TSS (5 folds)") +
  scale_x_continuous(breaks = seq(1, 10))
```

```{r}
kmeans_fv_3clusters <- kmeans_wflow_fv |>
  finalize_workflow_tidyclust(parameters = list(num_clusters = 3))
```

```{r}
set.seed(1002) 
# always reset the seed before you re-fit, just in case something weird happens

kmeans_fv_fit3 <- kmeans_fv_3clusters |>
  fit(data = vg_train)
```

```{r}
vg3 <- bind_cols(
  vg_train,
  kmeans_fv_fit3 |> extract_cluster_assignment())

vg3 |>
  select(Name, .cluster, everything())
```

```{r}
library(GGally)
ggpairs(vg3, columns = c("log_sales", "NA_log_sales", "EU_log_sales", "JP_log_sales", "OT_log_sales", "Platform", "Genre"),
        aes(color = .cluster))
```

```{r}
vg_predictions3 <- augment(kmeans_fv_fit3, 
                        new_data = vg_test)

ggpairs(vg_predictions3, columns = c("log_sales", "NA_log_sales", "EU_log_sales", "JP_log_sales", "OT_log_sales", "Platform", "Genre"),
        aes(color = .pred_cluster))
```

```{r}
vg_all_clusters3 <- bind_rows(
  vg3,
  vg_predictions3 |> rename(.cluster = .pred_cluster) # rename cluster variable name
)
```
