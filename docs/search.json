[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Logan Clark 437 Final",
    "section": "",
    "text": "Hello my name is Logan Clark and I’m a 4th year here at Cal State University Fullerton and this is my final project for my Math 437: Modern Data Analysis class."
  },
  {
    "objectID": "index.html#motivation-and-context",
    "href": "index.html#motivation-and-context",
    "title": "Logan Clark 437 Final",
    "section": "Motivation and Context",
    "text": "Motivation and Context\nFor my project I decided to look at the sales of Video Games. Video Games have been my main hobby for most of my life and I’ve always had an innate interest in sales numbers. I would go on Wikipedia just to look at the list of best selling games and see what changes would be happening on the chart on a weekly basis. Each game would have it’s own entry and show us what console it came out on, when it came out, etc. It makes me wonder if there’s any common trends among games that have higher sales overall, such as if a specific genre is more appealing, or if games on a certain console did better than others."
  },
  {
    "objectID": "index.html#main-objective",
    "href": "index.html#main-objective",
    "title": "Logan Clark 437 Final",
    "section": "Main Objective",
    "text": "Main Objective\nMy main Objective for this project is to use clustering to see what games are similar and to see if we can see any patterns among games in the cluster that contains higher global sales."
  },
  {
    "objectID": "index.html#packages-used-in-this-analysis",
    "href": "index.html#packages-used-in-this-analysis",
    "title": "Logan Clark 437 Final",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nlibrary(GGally)\nlibrary(tidyverse)\nlibrary(tidyclust)\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\n\nPackage\nUse\n\n\n\n\nGGally\nto easily load and save data\n\n\ntidyverse\nto import the CSV file data\n\n\ntidyclust\nto massage and summarize data\n\n\ntidymodels\nto split data into training and test sets\n\n\ndplyr\nto massage and summarize data\n\n\nggplot2\nto create nice-looking and informative graphs\n\n\nnanier\nto visualize missing data"
  },
  {
    "objectID": "index.html#data-description",
    "href": "index.html#data-description",
    "title": "Logan Clark 437 Final",
    "section": "Data Description",
    "text": "Data Description\n\nvgsales &lt;- readr::read_csv(\"C:/Users/19493/Desktop/vgsales.csv\")\n\nI got the dataset from a site called Kaggle.com and the data set analyzes sales data from over 16,000 games that had more than 100,000 global sales. The data itself was generated by a scrape of vgchartz.com, which is another website that goes into deep analysis about video games sales.\nThe Data has 11 different variables Rank is a games placement in sales, Name is the name of the game, Platform is which console said game came out on, Genre is the genre of the game, Publisher is who published the game, and the different Sales columns relate to how many copies the game sole in North America, Europe, Japan, all other territories, and final how many total copies were sold globally.\n\nData Limitations\nThe data only contains data until about 2016, so the data set itself is old and needs to be updated. There are a lot of outliers in the data itself, and about 2% of the year values are missing."
  },
  {
    "objectID": "index.html#data-wrangling",
    "href": "index.html#data-wrangling",
    "title": "Logan Clark 437 Final",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nvgsales2 &lt;- vgsales %&gt;%\n  filter(Global_Sales &gt; 1) %&gt;%\n  mutate(log_sales = log(Global_Sales)) %&gt;%\nmutate(NA_log_sales = log(NA_Sales + 0.0001)) %&gt;%\nmutate(EU_log_sales = log(EU_Sales + 0.0001)) %&gt;%\nmutate(JP_log_sales = log(JP_Sales + 0.0001)) %&gt;%\nmutate(OT_log_sales = log(Other_Sales + 0.0001)) %&gt;%\n  mutate(Platform = as.factor(Platform) |&gt;\nfct_collapse(\n  Sony = c(\"PS\", \"PS2\", \"PS3\", \"PS4\", \"PSP\", \"PSV\"),\n  Microsoft = c(\"X360\", \"XB\", \"XOne\"),\n  Nintendo = c(\"NES\", \"SNES\", \"Wii\", \"WiiU\", \"GC\", \"N64\", \"3DS\", \"DS\", \"GB\", \"GBA\"),\n  Other = c(\"2600\", \"DC\", \"GEN\", \"PC\", \"SAT\", \"SCD\")\n))\n\nvg_split &lt;- initial_split(vgsales2, prop = 0.75) \n\nvg_train &lt;- training(vg_split)\nvg_test &lt;- testing(vg_split)\n\nHere we have to makes some changes to the data so that our model later can run, and to help with outliers, we’ll call this new data something different so we can compare later. The main actions were to log all the Sales columns so that we wouldn’t have as many outliers, and to add a very small amount to the sales number since some values show up as 0.00, and that would mean if we log them we were get negative infinity. I also chose to include only games that sold a minimum of one million copies world wide so that we can see what similarities we see in games considered a success. We also change our Platform variable to only include 4 different values again so that our model can run later on. We can also split our data to see how well our training clusters do with the test set."
  },
  {
    "objectID": "index.html#eda",
    "href": "index.html#eda",
    "title": "Logan Clark 437 Final",
    "section": "EDA",
    "text": "EDA\n\nvgsales$Year[vgsales$Year == \"N/A\"] &lt;- NA\nvgsales$Publisher[vgsales$Publisher == \"N/A\"] &lt;- NA\n\nlibrary(naniar)\n\nWarning: package 'naniar' was built under R version 4.4.3\n\nvis_miss(vgsales[1:11])\n\n\n\n\n\n\n\n\n\nvgsales2$Year[vgsales2$Year == \"N/A\"] &lt;- NA\nvgsales2$Publisher[vgsales2$Publisher == \"N/A\"] &lt;- NA\nvis_miss(vgsales2[1:11])\n\n\n\n\n\n\n\n\nThese plots above show the missing data from our original data set and our wrangled data set. As we can see originally we had 2% of years missing and &lt;1% of Publisher missing. Now with our wrangled data we see that we only have ~1% of data missing from year. This doesn’t really affect our model later as we’re not going to use the year function to build it, but still important to know.\n\nggplot(data = vg_train,\n       aes(x = Year, y = Global_Sales)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(data = vg_train,\n       aes(x = Year, y = log_sales)) + geom_boxplot()\n\n\n\n\n\n\n\n\nAs we can see from the training data, the number of outliers in here is high, hence why earlier we took the log of the global sales column and made it it’s own variable. Our 1st graph shows the global sales unlogged and the second shows log sales, less outliers than before.\n\nggplot(data = vg_train,\n       aes(y = Global_Sales)) + geom_point(aes(x = NA_Sales), color = \"red\") + geom_smooth(aes(x = NA_Sales), color = \"red\", method = \"loess\") + geom_point(aes(x = EU_Sales), color = \"green\") + geom_smooth(aes(x = EU_Sales), color = \"green\", method = \"loess\") + geom_point(aes(x = JP_Sales), color = \"blue\") + geom_smooth(aes(x = JP_Sales), color = \"blue\", method = \"loess\") + geom_point(aes(x = Other_Sales), color = \"black\") + geom_smooth(aes(x = Other_Sales), color = \"black\", method = \"loess\") + labs(x = \"Sales by Region\", y = \"Global Sales\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "index.html#modeling",
    "href": "index.html#modeling",
    "title": "Logan Clark 437 Final",
    "section": "Modeling",
    "text": "Modeling\n\nkmeans_recipe_fv &lt;- recipe(~ log_sales + NA_log_sales + EU_log_sales + JP_log_sales + OT_log_sales + Platform + Genre, \n                           data = vg_train) |&gt;\n  step_YeoJohnson(all_numeric_predictors()) |&gt; # deal with skew issues\n  step_normalize(all_numeric_predictors()) |&gt; # deal with different variances\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt; \n  step_zv(all_predictors())\n\nThis is the recipe used for Kmeans clustering\n\nkmeans_model &lt;- k_means(num_clusters = tune()) |&gt;\n  set_args(nstart = 20)\n\nThe cluster model\n\nkmeans_wflow_fv &lt;- workflow() |&gt;\n  add_model(kmeans_model) |&gt;\n  add_recipe(kmeans_recipe_fv)\n\n\nset.seed(1002)\nfv_kfold_tidy &lt;- vfold_cv(vg_train, v = 5, repeats = 1) \nnclusters_grid &lt;- data.frame(num_clusters = seq(1, 10))\n\nkmeans_tuned_fv &lt;- tune_cluster(kmeans_wflow_fv,\n                                resamples = fv_kfold_tidy,\n                                metrics = cluster_metric_set(sse_total, \n                                                             sse_within_total, sse_ratio),\n                                grid = nclusters_grid)\n\ntuned_metrics &lt;- collect_metrics(kmeans_tuned_fv)\n\ntuned_metrics |&gt;\n  arrange(desc(.metric), num_clusters) |&gt;\n  select(num_clusters, .metric, mean, everything())\n\n# A tibble: 30 × 7\n   num_clusters .metric           mean .estimator     n std_err .config         \n          &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1            1 sse_within_total 8051. standard       5    1.81 Preprocessor1_M…\n 2            2 sse_within_total 6061. standard       5   20.0  Preprocessor1_M…\n 3            3 sse_within_total 5141. standard       5   13.6  Preprocessor1_M…\n 4            4 sse_within_total 4630. standard       5   20.2  Preprocessor1_M…\n 5            5 sse_within_total 4210. standard       5   23.7  Preprocessor1_M…\n 6            6 sse_within_total 3940. standard       5   21.3  Preprocessor1_M…\n 7            7 sse_within_total 3695. standard       5   21.7  Preprocessor1_M…\n 8            8 sse_within_total 3539. standard       5   20.6  Preprocessor1_M…\n 9            9 sse_within_total 3395. standard       5   17.4  Preprocessor1_M…\n10           10 sse_within_total 3265. standard       5   12.9  Preprocessor1_M…\n# ℹ 20 more rows\n\n\n\ntuned_metrics |&gt;\n  filter(.metric == \"sse_ratio\") |&gt;\n  ggplot(aes(x = num_clusters, y = mean)) +\n  geom_point() + \n  geom_line() +\n  labs(x = \"Number of Clusters\", y = \"Mean WSS/TSS (5 folds)\") +\n  scale_x_continuous(breaks = seq(1, 10))\n\n\n\n\n\n\n\n\n\nkmeans_fv_3clusters &lt;- kmeans_wflow_fv |&gt;\n  finalize_workflow_tidyclust(parameters = list(num_clusters = 3))\n\n\nset.seed(1002) \n# always reset the seed before you re-fit, just in case something weird happens\n\nkmeans_fv_fit3 &lt;- kmeans_fv_3clusters |&gt;\n  fit(data = vg_train)\n\n\nvg3 &lt;- bind_cols(\n  vg_train,\n  kmeans_fv_fit3 |&gt; extract_cluster_assignment())\n\nvg3 |&gt;\n  select(Name, .cluster, everything())\n\n# A tibble: 1,540 × 17\n   Name          .cluster  Rank Platform Year  Genre Publisher NA_Sales EU_Sales\n   &lt;chr&gt;         &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 Chrono Trigg… Cluster…   690 Nintendo 1995  Role… SquareSo…     0.28     0   \n 2 Need for Spe… Cluster…   731 Microso… 2010  Raci… Electron…     1.03     0.98\n 3 My Fitness C… Cluster…   679 Nintendo 2008  Spor… Ubisoft       1.2      0.9 \n 4 Tiger Woods … Cluster…  1130 Sony     2003  Spor… Electron…     1.18     0.34\n 5 Super Mario … Cluster…  1146 Nintendo 2005  Spor… Nintendo      0.98     0.38\n 6 Madden NFL 25 Cluster…   984 Sony     2013  Spor… Electron…     1.59     0.03\n 7 LEGO Star Wa… Cluster…  1669 Sony     2011  Acti… LucasArts     0.52     0.49\n 8 Titanfall     Cluster…   495 Microso… 2014  Shoo… Electron…     1.84     0.8 \n 9 Yakuza 3      Cluster…  1815 Sony     2009  Acti… Sega          0.21     0.21\n10 Crash Bandic… Cluster…  1426 Nintendo 2003  Plat… Vivendi …     0.63     0.66\n# ℹ 1,530 more rows\n# ℹ 8 more variables: JP_Sales &lt;dbl&gt;, Other_Sales &lt;dbl&gt;, Global_Sales &lt;dbl&gt;,\n#   log_sales &lt;dbl&gt;, NA_log_sales &lt;dbl&gt;, EU_log_sales &lt;dbl&gt;,\n#   JP_log_sales &lt;dbl&gt;, OT_log_sales &lt;dbl&gt;\n\n\n\nlibrary(GGally)\nggpairs(vg3, columns = c(\"log_sales\", \"NA_log_sales\", \"EU_log_sales\", \"JP_log_sales\", \"OT_log_sales\", \"Platform\", \"Genre\"),\n        aes(color = .cluster))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nvg_predictions3 &lt;- augment(kmeans_fv_fit3, \n                        new_data = vg_test)\n\nggpairs(vg_predictions3, columns = c(\"log_sales\", \"NA_log_sales\", \"EU_log_sales\", \"JP_log_sales\", \"OT_log_sales\", \"Platform\", \"Genre\"),\n        aes(color = .pred_cluster))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nvg_all_clusters3 &lt;- bind_rows(\n  vg3,\n  vg_predictions3 |&gt; rename(.cluster = .pred_cluster) # rename cluster variable name\n)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Data Wrangling and Modeling",
    "section": "",
    "text": "library(GGally)\nlibrary(tidyverse)\nlibrary(tidyclust)\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nGGally to get the ggpairs function\ntidyclust to have a tidy interface for clustering models\ndplyr to massage and summarize data\ntidymodels for modeling\ntidyverse for piping and altering data\nggplot2 for graphs in eda and clustering"
  },
  {
    "objectID": "about.html#packages-used-in-this-analysis",
    "href": "about.html#packages-used-in-this-analysis",
    "title": "Data Wrangling and Modeling",
    "section": "",
    "text": "library(GGally)\nlibrary(tidyverse)\nlibrary(tidyclust)\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nGGally to get the ggpairs function\ntidyclust to have a tidy interface for clustering models\ndplyr to massage and summarize data\ntidymodels for modeling\ntidyverse for piping and altering data\nggplot2 for graphs in eda and clustering"
  },
  {
    "objectID": "about.html#data-description",
    "href": "about.html#data-description",
    "title": "Data Wrangling and Modeling",
    "section": "Data Description",
    "text": "Data Description\n\nvgsales &lt;- readr::read_csv(\"C:/Users/19493/Desktop/vgsales.csv\")\n\nI got the dataset from a site called Kaggle.com and the data set analyzes sales data from over 16,000 games that had more than 100,000 global sales. The data itself was generated by a scrape of vgchartz.com, which is another website that goes into deep analysis about video games sales.\nThe Data has 11 different variables Rank is a games placement in sales, Name is the name of the game, Platform is which console said game came out on, Genre is the genre of the game, Publisher is who published the game, and the different Sales columns relate to how many copies the game sole in North America, Europe, Japan, all other territories, and final how many total copies were sold globally.\n\nData Limitations\nThe data only contains data until about 2016, so the data set itself is old and needs to be updated. There are a lot of outliers in the data itself (as you’ll see in a bit in EDA."
  },
  {
    "objectID": "about.html#eda",
    "href": "about.html#eda",
    "title": "Data Wrangling and Modeling",
    "section": "EDA",
    "text": "EDA\n\nvgsales &lt;- vgsales %&gt;%\n  filter(Global_Sales &gt; 1) %&gt;%\n  mutate(log_sales = log(Global_Sales)) %&gt;%\nmutate(NA_log_sales = log(NA_Sales + 0.01)) %&gt;%\nmutate(EU_log_sales = log(EU_Sales + 0.01)) %&gt;%\nmutate(JP_log_sales = log(JP_Sales + 0.01)) %&gt;%\nmutate(OT_log_sales = log(Other_Sales + 0.1)) %&gt;%\n  mutate(Platform = as.factor(Platform) |&gt;\nfct_collapse(\n  Sony = c(\"PS\", \"PS2\", \"PS3\", \"PS4\", \"PSP\", \"PSV\"),\n  Microsoft = c(\"X360\", \"XB\", \"XOne\"),\n  Nintendo = c(\"NES\", \"SNES\", \"Wii\", \"WiiU\", \"GC\", \"N64\", \"3DS\", \"DS\", \"GB\", \"GBA\"),\n  Other = c(\"2600\", \"DC\", \"GEN\", \"PC\", \"SAT\", \"SCD\")\n))\n\n\nggplot(data = vgsales,\n       aes(x = Year, y = Global_Sales)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(data = vgsales,\n       aes(x = Year, y = log_sales)) + geom_boxplot()"
  },
  {
    "objectID": "about.html#modeling",
    "href": "about.html#modeling",
    "title": "Data Wrangling and Modeling",
    "section": "Modeling",
    "text": "Modeling\n\nvg_split &lt;- initial_split(vgsales, prop = 0.75) \n\nvg_train &lt;- training(vg_split)\nvg_test &lt;- testing(vg_split)\n\n\nkmeans_recipe_fv &lt;- recipe(~ log_sales + NA_log_sales + EU_log_sales + JP_log_sales + OT_log_sales + Platform + Genre, \n                           data = vg_train) |&gt;\n  step_YeoJohnson(all_numeric_predictors()) |&gt; # deal with skew issues\n  step_normalize(all_numeric_predictors()) |&gt; # deal with different variances\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt; \n  step_zv(all_predictors())\n\nThis is the recipe used for Kmeans clustering\n\nkmeans_model &lt;- k_means(num_clusters = tune()) |&gt;\n  set_args(nstart = 20)\n\nThe cluster model\n\nkmeans_wflow_fv &lt;- workflow() |&gt;\n  add_model(kmeans_model) |&gt;\n  add_recipe(kmeans_recipe_fv)\n\n\nset.seed(1002)\nfv_kfold_tidy &lt;- vfold_cv(vg_train, v = 5, repeats = 1) \nnclusters_grid &lt;- data.frame(num_clusters = seq(1, 10))\n\nkmeans_tuned_fv &lt;- tune_cluster(kmeans_wflow_fv,\n                                resamples = fv_kfold_tidy,\n                                metrics = cluster_metric_set(sse_total, \n                                                             sse_within_total, sse_ratio),\n                                grid = nclusters_grid)\n\n! Fold1: preprocessor 1/1, model 9/10: did not converge in 10 iterations\n\ntuned_metrics &lt;- collect_metrics(kmeans_tuned_fv)\n\ntuned_metrics |&gt;\n  arrange(desc(.metric), num_clusters) |&gt;\n  select(num_clusters, .metric, mean, everything())\n\n# A tibble: 30 × 7\n   num_clusters .metric           mean .estimator     n std_err .config         \n          &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1            1 sse_within_total 8053. standard       5    1.37 Preprocessor1_M…\n 2            2 sse_within_total 5870. standard       5   13.7  Preprocessor1_M…\n 3            3 sse_within_total 4883. standard       5    4.38 Preprocessor1_M…\n 4            4 sse_within_total 4431. standard       5    6.78 Preprocessor1_M…\n 5            5 sse_within_total 4079. standard       5    5.64 Preprocessor1_M…\n 6            6 sse_within_total 3803. standard       5    5.80 Preprocessor1_M…\n 7            7 sse_within_total 3572. standard       5    4.36 Preprocessor1_M…\n 8            8 sse_within_total 3400. standard       5    8.45 Preprocessor1_M…\n 9            9 sse_within_total 3253. standard       5    5.96 Preprocessor1_M…\n10           10 sse_within_total 3125. standard       5    4.90 Preprocessor1_M…\n# ℹ 20 more rows\n\n\n\ntuned_metrics |&gt;\n  filter(.metric == \"sse_ratio\") |&gt;\n  ggplot(aes(x = num_clusters, y = mean)) +\n  geom_point() + \n  geom_line() +\n  labs(x = \"Number of Clusters\", y = \"Mean WSS/TSS (5 folds)\") +\n  scale_x_continuous(breaks = seq(1, 10))\n\n\n\n\n\n\n\n\n\nkmeans_fv_3clusters &lt;- kmeans_wflow_fv |&gt;\n  finalize_workflow_tidyclust(parameters = list(num_clusters = 3))\n\n\nset.seed(1002) \n# always reset the seed before you re-fit, just in case something weird happens\n\nkmeans_fv_fit3 &lt;- kmeans_fv_3clusters |&gt;\n  fit(data = vg_train)\n\n\nvg3 &lt;- bind_cols(\n  vg_train,\n  kmeans_fv_fit3 |&gt; extract_cluster_assignment())\n\nvg3 |&gt;\n  select(Name, .cluster, everything())\n\n# A tibble: 1,540 × 17\n   Name          .cluster  Rank Platform Year  Genre Publisher NA_Sales EU_Sales\n   &lt;chr&gt;         &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 Game Party 3  Cluster…  1043 Nintendo 2009  Puzz… Warner B…     1.43     0.16\n 2 Grand Theft … Cluster…   875 Microso… 2005  Acti… Take-Two…     1.26     0.61\n 3 Fable         Cluster…   569 Microso… 2004  Role… Microsof…     1.99     0.58\n 4 Call of Duty… Cluster…  1128 Other    N/A   Shoo… Activisi…     0.58     0.81\n 5 Donkey Kong … Cluster…    72 Nintendo 1994  Plat… Nintendo      4.36     1.71\n 6 Imagine: Wed… Cluster…  1542 Nintendo 2008  Simu… Ubisoft       0.55     0.59\n 7 Pac-Man Worl… Cluster…  1242 Sony     2002  Acti… Namco Ba…     1.26     0.05\n 8 Max Payne 2:… Cluster…  1464 Sony     2003  Shoo… Take-Two…     0.65     0.51\n 9 Mortal Komba… Cluster…   620 Sony     2002  Figh… Midway G…     1.81     0.52\n10 The Lord of … Cluster…  1797 Microso… 2003  Acti… Electron…     0.71     0.38\n# ℹ 1,530 more rows\n# ℹ 8 more variables: JP_Sales &lt;dbl&gt;, Other_Sales &lt;dbl&gt;, Global_Sales &lt;dbl&gt;,\n#   log_sales &lt;dbl&gt;, NA_log_sales &lt;dbl&gt;, EU_log_sales &lt;dbl&gt;,\n#   JP_log_sales &lt;dbl&gt;, OT_log_sales &lt;dbl&gt;\n\n\n\nlibrary(GGally)\nggpairs(vg3, columns = c(\"log_sales\", \"NA_log_sales\", \"EU_log_sales\", \"JP_log_sales\", \"OT_log_sales\", \"Platform\", \"Genre\"),\n        aes(color = .cluster))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nvg_predictions3 &lt;- augment(kmeans_fv_fit3, \n                        new_data = vg_test)\n\nggpairs(vg_predictions3, columns = c(\"log_sales\", \"NA_log_sales\", \"EU_log_sales\", \"JP_log_sales\", \"OT_log_sales\", \"Platform\", \"Genre\"),\n        aes(color = .pred_cluster))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nvg_all_clusters3 &lt;- bind_rows(\n  vg3,\n  vg_predictions3 |&gt; rename(.cluster = .pred_cluster) # rename cluster variable name\n)"
  },
  {
    "objectID": "about.html#insights",
    "href": "about.html#insights",
    "title": "Data Wrangling and Modeling",
    "section": "Insights",
    "text": "Insights\n\nggplot(data = vg_all_clusters3, mapping = aes(x = log_sales, fill = .cluster)) + geom_density()\n\n\n\n\n\n\n\n\n\nggplot(data = vg_all_clusters3, mapping = aes(x = NA_log_sales, fill = .cluster)) + geom_density()\n\n\n\n\n\n\n\n\n\nggplot(data = vg_all_clusters3, mapping = aes(x = EU_log_sales, fill = .cluster)) + geom_density()\n\n\n\n\n\n\n\n\n\nggplot(data = vg_all_clusters3, mapping = aes(x = JP_log_sales, fill = .cluster)) + geom_density()\n\n\n\n\n\n\n\n\n\nggplot(data = vg_all_clusters3, mapping = aes(x = OT_log_sales, fill = .cluster)) + geom_density()\n\n\n\n\n\n\n\n\n\nggplot(data = vg_all_clusters3, mapping = aes(x = Platform, fill = .cluster)) + geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(data = vg_all_clusters3, mapping = aes(x = Genre, fill = .cluster)) + geom_bar()"
  },
  {
    "objectID": "Final-Project/Codes.html",
    "href": "Final-Project/Codes.html",
    "title": "Code",
    "section": "",
    "text": "Code Chunks for the website"
  }
]